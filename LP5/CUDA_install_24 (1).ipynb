{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgmu4daT47EX",
        "outputId": "d413dabe-4762-4258-8354-7a566ca0de31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPYBE7yD6QZw",
        "outputId": "2837f526-aaf7-4c4b-e05c-1f93d8514fec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-buior6cd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-buior6cd\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 5741c522547756ac4bb7a16df32106a15efb8a57\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10742 sha256=8f76f24c8ef76f4567737825c96eef1bce4cf6e61dca70f1aea8c5c7f7a278f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xp2lf6df/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12nOStMv6UuV",
        "outputId": "f34a0792-cb5d-4e51-a3ea-51ef42c7f505"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpgcs4ecg8\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <cstdio>\n",
        "#include <iostream>\n",
        "\n",
        "\tusing namespace std;\n",
        "\n",
        "__global__ void maxi(int* a, int* b, int n)\n",
        "{\n",
        "\tint block = 256 * blockIdx.x;\n",
        "\tint max = 0;\n",
        "\n",
        "\tfor (int i = block; i < min(256 + block, n); i++) {\n",
        "\n",
        "\t\tif (max < a[i]) {\n",
        "\t\t\tmax = a[i];\n",
        "\t\t}\n",
        "\t}\n",
        "\tb[blockIdx.x] = max;\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "\tint n;\n",
        "\tn = 3 >> 2;\n",
        "\tint a[n];\n",
        "\n",
        "\tfor (int i = 0; i < n; i++) {\n",
        "\t\ta[i] = rand() % n;\n",
        "\t\tcout << a[i] << \"\\t\";\n",
        "\t}\n",
        "\n",
        "\tcudaEvent_t start, end;\n",
        "\tint *ad, *bd;\n",
        "\tint size = n * sizeof(int);\n",
        "\tcudaMalloc(&ad, size);\n",
        "\tcudaMemcpy(ad, a, size, cudaMemcpyHostToDevice);\n",
        "\tint grids = ceil(n * 1.0f / 256.0f);\n",
        "\tcudaMalloc(&bd, grids * sizeof(int));\n",
        "\n",
        "\tdim3 grid(grids, 1);\n",
        "\tdim3 block(1, 1);\n",
        "\n",
        "\tcudaEventCreate(&start);\n",
        "\tcudaEventCreate(&end);\n",
        "\tcudaEventRecord(start);\n",
        "\n",
        "\twhile (n > 1) {\n",
        "\t\tmaxi<<<grids, block>>>(ad, bd, n);\n",
        "\t\tn = ceil(n * 1.0f / 256.0f);\n",
        "\t\tcudaMemcpy(ad, bd, n * sizeof(int), cudaMemcpyDeviceToDevice);\n",
        "\t}\n",
        "\n",
        "\tcudaEventRecord(end);\n",
        "\tcudaEventSynchronize(end);\n",
        "\n",
        "\tfloat time = 0;\n",
        "\tcudaEventElapsedTime(&time, start, end);\n",
        "\n",
        "\tint ans[2];\n",
        "\tcudaMemcpy(ans, ad, 4, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcout << \"The maximum element is : \" << ans[0] << endl;\n",
        "\n",
        "\tcout << \"The time required : \";\n",
        "\tcout << time << endl;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36dQqV516WkA",
        "outputId": "84a1141b-4264-4e94-9d67-4248701e85b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The maximum element is : 0\n",
            "The time required : 0.004096\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "\n",
        "// CUDA kernel for parallel reduction to find the minimum value in an array\n",
        "__global__ void minReduce(const int* arr, int* result, int N) {\n",
        "    extern __shared__ int sharedData[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sharedData[tid] = (i < N) ? arr[i] : INT_MAX;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s && i + s < N) {\n",
        "            sharedData[tid] = min(sharedData[tid], sharedData[tid + s]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for parallel reduction to find the maximum value in an array\n",
        "__global__ void maxReduce(const int* arr, int* result, int N) {\n",
        "    extern __shared__ int sharedData[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sharedData[tid] = (i < N) ? arr[i] : INT_MIN;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s && i + s < N) {\n",
        "            sharedData[tid] = max(sharedData[tid], sharedData[tid + s]);\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "// CUDA kernel for parallel reduction to find the sum of values in an array\n",
        "__global__ void sumReduce(const int* arr, int* result, int N) {\n",
        "    extern __shared__ int sharedData[];\n",
        "\n",
        "    int tid = threadIdx.x;\n",
        "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    sharedData[tid] = (i < N) ? arr[i] : 0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n",
        "        if (tid < s && i + s < N) {\n",
        "            sharedData[tid] += sharedData[tid + s];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        result[blockIdx.x] = sharedData[0];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 512; // Number of elements in the array\n",
        "    const int blockSize = 256; // Threads per block\n",
        "\n",
        "    // Generate random array\n",
        "    std::vector<int> hostArr(N);\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        hostArr[i] = rand() % 1000; // Generate random numbers between 0 to 999\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    int *deviceArr, *deviceResult;\n",
        "    cudaMalloc((void**)&deviceArr, N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceResult, sizeof(int));\n",
        "\n",
        "    // Copy data from host to device\n",
        "    cudaMemcpy(deviceArr, hostArr.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Compute grid size\n",
        "    int gridSize = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Find minimum\n",
        "    minReduce<<<gridSize, blockSize, blockSize * sizeof(int)>>>(deviceArr, deviceResult, N);\n",
        "    int minVal;\n",
        "    cudaMemcpy(&minVal, deviceResult, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Minimum: \" << minVal << std::endl;\n",
        "\n",
        "    // Find maximum\n",
        "    maxReduce<<<gridSize, blockSize, blockSize * sizeof(int)>>>(deviceArr, deviceResult, N);\n",
        "    int maxVal;\n",
        "    cudaMemcpy(&maxVal, deviceResult, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Maximum: \" << maxVal << std::endl;\n",
        "\n",
        "    // Find sum\n",
        "    sumReduce<<<gridSize, blockSize, blockSize * sizeof(int)>>>(deviceArr, deviceResult, N);\n",
        "    int sumVal;\n",
        "    cudaMemcpy(&sumVal, deviceResult, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    std::cout << \"Sum: \" << sumVal << std::endl;\n",
        "\n",
        "    // Find average\n",
        "    double average = static_cast<double>(sumVal) / N;\n",
        "    std::cout << \"Average: \" << average << std::endl;\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(deviceArr);\n",
        "    cudaFree(deviceResult);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "-06nwLgX6ssI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2edf582-29fb-4225-8319-3f07c1913792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum: 11\n",
            "Maximum: 996\n",
            "Sum: 131654\n",
            "Average: 257.137\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function for vector addition\n",
        "__global__ void vectorAdd(int* a, int* b, int* c, int n) {\n",
        "    int index = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (index < n) {\n",
        "        c[index] = a[index] + b[index];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000000; // Size of the vectors\n",
        "    const int blockSize = 256; // Threads per block\n",
        "    const int gridSize = (N + blockSize - 1) / blockSize; // Number of blocks\n",
        "\n",
        "    // Initialize host vectors\n",
        "    std::vector<int> hostA(N);\n",
        "    std::vector<int> hostB(N);\n",
        "    std::vector<int> hostC(N);\n",
        "\n",
        "    // Fill host vectors with random values\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        hostA[i] = rand() % 1000;\n",
        "        hostB[i] = rand() % 1000;\n",
        "    }\n",
        "\n",
        "    // Declare device pointers\n",
        "    int *deviceA, *deviceB, *deviceC;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&deviceA, N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceB, N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceC, N * sizeof(int));\n",
        "\n",
        "    // Copy host data to device\n",
        "    cudaMemcpy(deviceA, hostA.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    vectorAdd<<<gridSize, blockSize>>>(deviceA, deviceB, deviceC, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(hostC.data(), deviceC, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify results\n",
        "    bool success = true;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        if (hostC[i] != hostA[i] + hostB[i]) {\n",
        "            success = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (success) {\n",
        "        std::cout << \"Vector addition successful!\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Vector addition failed!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(deviceA);\n",
        "    cudaFree(deviceB);\n",
        "    cudaFree(deviceC);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_tgbiA0JBZC",
        "outputId": "3c8430f9-8265-4a52-95bf-2b2d45f1f437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector addition successful!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function for matrix multiplication\n",
        "__global__ void matrixMul(const int* A, const int* B, int* C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        int sum = 0;\n",
        "        for (int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000; // Size of the matrices\n",
        "    const int blockSize = 16; // Threads per block\n",
        "    const int gridSize = (N + blockSize - 1) / blockSize; // Number of blocks\n",
        "\n",
        "    // Initialize host matrices\n",
        "    std::vector<int> hostA(N * N);\n",
        "    std::vector<int> hostB(N * N);\n",
        "    std::vector<int> hostC(N * N);\n",
        "\n",
        "    // Fill host matrices with random values\n",
        "    for (int i = 0; i < N * N; ++i) {\n",
        "        hostA[i] = rand() % 10;\n",
        "        hostB[i] = rand() % 10;\n",
        "    }\n",
        "\n",
        "    // Declare device pointers\n",
        "    int *deviceA, *deviceB, *deviceC;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&deviceA, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceB, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceC, N * N * sizeof(int));\n",
        "\n",
        "    // Copy host data to device\n",
        "    cudaMemcpy(deviceA, hostA.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    dim3 threadsPerBlock(blockSize, blockSize);\n",
        "    dim3 numBlocks(gridSize, gridSize);\n",
        "    matrixMul<<<numBlocks, threadsPerBlock>>>(deviceA, deviceB, deviceC, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(hostC.data(), deviceC, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Verify results\n",
        "    bool success = true;\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            int sum = 0;\n",
        "            for (int k = 0; k < N; ++k) {\n",
        "                sum += hostA[i * N + k] * hostB[k * N + j];\n",
        "            }\n",
        "            if (hostC[i * N + j] != sum) {\n",
        "                success = false;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        if (!success) {\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (success) {\n",
        "        std::cout << \"Matrix multiplication successful!\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Matrix multiplication failed!\" << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(deviceA);\n",
        "    cudaFree(deviceB);\n",
        "    cudaFree(deviceC);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGlbLKPeJlaY",
        "outputId": "ebb67549-fbac-44f2-ed6b-44283da55b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication successful!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Kernel function for matrix multiplication\n",
        "__global__ void matrixMul(const int* A, const int* B, int* C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        int sum = 0;\n",
        "        for (int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to print a matrix\n",
        "void printMatrix(const std::vector<int>& matrix, int N) {\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        for (int j = 0; j < N; ++j) {\n",
        "            std::cout << matrix[i * N + j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 3; // Size of the matrices (for demonstration purposes)\n",
        "    const int blockSize = 2; // Threads per block\n",
        "    const int gridSize = (N + blockSize - 1) / blockSize; // Number of blocks\n",
        "\n",
        "    // Initialize host matrices\n",
        "    std::vector<int> hostA = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
        "    std::vector<int> hostB = {9, 8, 7, 6, 5, 4, 3, 2, 1};\n",
        "    std::vector<int> hostC(N * N);\n",
        "\n",
        "    // Print input matrices\n",
        "    std::cout << \"Matrix A:\" << std::endl;\n",
        "    printMatrix(hostA, N);\n",
        "    std::cout << \"Matrix B:\" << std::endl;\n",
        "    printMatrix(hostB, N);\n",
        "\n",
        "    // Declare device pointers\n",
        "    int *deviceA, *deviceB, *deviceC;\n",
        "\n",
        "    // Allocate device memory\n",
        "    cudaMalloc((void**)&deviceA, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceB, N * N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceC, N * N * sizeof(int));\n",
        "\n",
        "    // Copy host data to device\n",
        "    cudaMemcpy(deviceA, hostA.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB.data(), N * N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel\n",
        "    dim3 threadsPerBlock(blockSize, blockSize);\n",
        "    dim3 numBlocks(gridSize, gridSize);\n",
        "    matrixMul<<<numBlocks, threadsPerBlock>>>(deviceA, deviceB, deviceC, N);\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(hostC.data(), deviceC, N * N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print result matrix\n",
        "    std::cout << \"Matrix C (A + B):\" << std::endl;\n",
        "    printMatrix(hostC, N);\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(deviceA);\n",
        "    cudaFree(deviceB);\n",
        "    cudaFree(deviceC);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5hySWGZKBhi",
        "outputId": "bccd9cf7-7e9c-4cc7-816e-34684e2da1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            "1 2 3 \n",
            "4 5 6 \n",
            "7 8 9 \n",
            "Matrix B:\n",
            "9 8 7 \n",
            "6 5 4 \n",
            "3 2 1 \n",
            "Matrix C (A + B):\n",
            "30 24 18 \n",
            "84 69 54 \n",
            "138 114 90 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "\n",
        "// CUDA kernel to add two vectors element-wise\n",
        "__global__ void vectorAdd(const int *a, const int *b, int *c, int size) {\n",
        "    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (tid < size) {\n",
        "        c[tid] = a[tid] + b[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int N = 1000; // Size of the vectors\n",
        "    const int blockSize = 256; // Threads per block\n",
        "\n",
        "    // Generate random vectors\n",
        "    std::vector<int> hostA(N);\n",
        "    std::vector<int> hostB(N);\n",
        "    std::vector<int> hostC(N);\n",
        "\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        hostA[i] = rand() % 1000; // Random numbers between 0 and 999\n",
        "        hostB[i] = rand() % 1000;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    int *deviceA, *deviceB, *deviceC;\n",
        "    cudaMalloc((void**)&deviceA, N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceB, N * sizeof(int));\n",
        "    cudaMalloc((void**)&deviceC, N * sizeof(int));\n",
        "\n",
        "    // Copy input vectors from host to device memory\n",
        "    cudaMemcpy(deviceA, hostA.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(deviceB, hostB.data(), N * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Compute grid size\n",
        "    int gridSize = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "    // Launch the kernel\n",
        "    vectorAdd<<<gridSize, blockSize>>>(deviceA, deviceB, deviceC, N);\n",
        "\n",
        "    // Copy result from device to host memory\n",
        "    cudaMemcpy(hostC.data(), deviceC, N * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print original vectors and their sum vector\n",
        "    std::cout << \"Vector A: \";\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        std::cout << hostA[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    std::cout << \"Vector B: \";\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        std::cout << hostB[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    std::cout << \"Sum Vector: \";\n",
        "    for (int i = 0; i < N; ++i) {\n",
        "        std::cout << hostC[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(deviceA);\n",
        "    cudaFree(deviceB);\n",
        "    cudaFree(deviceC);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX8mClaJKcfu",
        "outputId": "fdced196-36fb-4c95-cc53-3abcf606a65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector A: 697 474 491 133 578 201 683 133 400 101 845 581 702 693 894 959 406 817 871 609 702 313 888 649 408 365 684 540 856 946 138 342 339 442 166 363 565 606 826 93 710 840 757 485 613 715 337 648 922 943 845 309 983 216 876 396 120 37 616 194 306 684 671 24 478 34 273 86 963 770 963 482 754 664 42 303 120 344 24 737 687 530 818 921 270 325 561 669 327 37 544 70 32 807 135 262 667 136 760 620 805 616 753 34 970 801 468 530 287 260 586 597 667 92 495 810 9 18 63 546 180 252 572 500 824 985 634 17 20 81 525 519 669 353 646 469 247 474 856 382 86 443 303 22 625 895 698 389 656 803 641 129 346 764 282 587 867 90 195 197 920 675 859 690 158 818 470 744 608 723 8 611 693 397 518 764 391 760 855 117 648 869 588 287 826 438 673 61 455 958 109 115 514 750 30 439 155 797 626 643 447 991 836 839 567 969 225 176 312 813 145 373 265 748 959 48 178 879 147 657 221 742 63 889 548 70 660 699 318 169 191 36 312 735 338 526 371 709 539 514 6 485 732 591 984 680 681 249 336 805 81 938 951 591 950 634 42 974 304 177 382 167 926 409 753 641 249 639 282 747 688 541 501 857 138 797 892 584 733 785 264 24 709 419 599 577 548 370 690 206 613 349 671 29 983 242 593 114 635 998 699 873 398 648 174 917 561 646 551 989 489 135 203 441 782 424 367 81 245 205 288 207 740 434 880 531 360 508 915 753 741 530 826 577 341 843 609 830 608 345 393 210 542 20 668 506 302 573 0 256 285 674 853 947 474 950 169 324 879 873 469 13 121 955 89 815 662 972 346 977 213 391 792 150 763 923 108 707 876 54 578 692 63 109 936 934 718 236 745 533 762 741 98 155 815 291 431 987 352 256 886 142 219 466 116 473 161 805 366 585 171 280 564 801 67 556 951 262 428 135 432 480 174 204 598 426 370 76 677 202 638 50 809 463 392 486 558 770 604 977 116 324 814 772 981 588 85 264 654 483 326 223 640 321 321 151 405 628 931 63 727 235 993 58 578 45 567 493 967 156 816 429 486 145 810 681 563 701 119 907 4 954 925 132 965 532 948 707 371 852 764 70 154 939 882 971 150 811 77 320 231 528 519 966 791 424 55 938 618 483 723 91 84 654 197 205 531 312 673 941 800 724 211 417 892 34 59 193 380 305 526 358 948 108 653 437 118 14 528 863 684 387 374 886 132 684 406 68 417 327 929 877 630 728 674 930 488 453 955 969 236 450 48 349 694 888 145 22 8 638 694 393 591 749 539 337 76 769 305 294 147 326 300 420 803 991 980 383 584 821 556 250 946 461 138 955 289 929 334 250 113 986 978 16 854 37 347 747 307 601 667 434 541 898 769 364 741 975 424 634 532 393 311 877 52 767 386 27 621 155 425 273 574 591 944 629 249 60 218 531 120 693 556 139 350 568 518 646 145 795 815 270 150 625 324 342 160 304 108 246 441 937 359 757 173 946 329 405 250 552 891 372 930 472 823 200 884 576 73 451 238 587 669 102 727 847 549 374 599 573 916 38 910 99 652 935 652 161 219 593 270 916 363 354 840 736 700 908 94 299 910 341 812 235 118 329 621 912 592 947 194 189 811 210 809 355 799 633 551 995 698 762 274 656 294 300 640 439 643 60 862 524 112 625 379 36 296 100 826 790 713 312 283 508 753 732 687 981 766 166 578 926 2 446 136 300 30 554 716 944 334 790 285 525 730 901 246 183 206 780 569 543 45 617 215 583 590 275 497 630 843 535 63 42 700 316 255 573 13 414 104 130 75 921 205 151 787 842 757 471 60 432 351 968 548 944 83 698 479 82 874 437 272 863 868 191 826 948 981 245 650 885 616 892 554 132 560 442 905 213 344 406 380 404 622 946 192 357 766 256 928 101 662 24 200 45 346 68 157 792 474 755 274 179 714 361 528 482 357 110 447 455 958 873 699 950 108 544 818 776 863 526 952 934 218 571 904 811 901 34 54 479 690 249 99 96 515 57 334 480 741 922 430 860 122 637 232 42 139 487 21 256 558 59 306 149 944 44 156 58 152 806 163 10 197 597 81 459 168 883 410 225 455 81 526 242 539 766 602 709 274 721 161 346 51 507 614 872 1 34 796 262 677 742 806 430 758 788 684 952 68 253 191 105 430 777 767 266 68 724 459 659 659 841 285 783 605 389 161 694 985 169 510 197 443 157 133 177 319 839 164 311 658 271 898 762 788 987 719 581 \n",
            "Vector B: 500 225 437 359 48 512 725 912 258 725 582 934 557 207 553 592 434 897 302 449 162 737 798 288 102 253 946 386 233 750 905 544 512 562 51 869 676 453 608 234 458 747 380 965 431 103 410 28 442 88 306 410 268 161 309 938 236 229 523 581 909 643 684 593 967 323 695 608 302 191 518 83 519 370 858 349 339 791 368 854 771 313 616 782 43 233 807 315 333 369 692 664 415 408 544 823 792 486 409 30 945 366 285 80 423 514 872 500 690 774 522 606 85 427 64 652 426 114 404 385 348 648 134 211 760 411 935 301 461 867 243 886 890 84 899 826 721 171 974 32 368 721 812 675 455 502 414 719 742 303 625 241 956 554 147 369 382 522 464 172 92 970 416 515 845 151 300 817 861 890 310 875 53 240 946 790 439 250 802 365 936 118 965 548 11 186 402 366 810 325 75 500 228 722 867 678 660 95 436 804 434 121 404 291 150 676 84 91 278 343 605 653 522 243 392 406 391 14 71 714 626 798 270 375 55 46 796 925 447 277 569 369 267 459 745 911 268 786 951 439 584 666 184 51 112 527 68 993 984 26 683 805 999 842 105 308 119 774 918 640 858 64 416 614 566 834 931 553 583 233 381 730 868 157 34 520 964 171 346 838 778 514 16 992 166 287 470 49 227 828 355 506 285 404 814 248 619 654 533 234 575 247 595 88 206 788 266 233 675 535 584 434 249 838 439 481 297 117 729 419 122 849 853 644 776 722 666 563 949 697 529 460 611 423 134 981 402 349 617 488 876 105 719 809 773 761 37 480 150 693 99 246 505 461 293 219 160 64 344 900 732 123 587 121 211 726 489 516 920 172 279 734 67 671 24 232 289 581 15 965 221 241 16 409 438 501 325 510 386 508 809 222 800 974 210 746 335 494 448 288 542 956 378 964 352 231 214 259 961 534 228 954 619 916 409 953 698 170 728 897 608 593 285 892 141 121 58 934 229 744 758 941 312 589 598 825 318 84 975 202 755 486 752 491 974 971 990 894 727 46 322 236 994 596 650 789 827 571 400 264 3 812 851 647 893 256 621 208 166 288 307 573 57 417 560 537 268 908 759 698 304 572 799 892 288 133 962 434 204 181 533 679 771 273 846 239 278 76 943 43 205 531 587 890 170 907 494 209 877 852 806 226 902 162 326 781 85 50 630 464 365 208 662 181 252 967 125 143 199 577 862 57 23 621 401 806 513 329 431 420 897 743 933 754 544 658 42 354 176 71 117 47 891 510 591 358 670 215 692 439 653 642 518 465 676 975 765 128 102 683 569 534 847 898 71 775 787 476 118 351 182 292 700 308 59 497 385 923 485 123 158 984 19 603 755 637 633 589 702 137 241 93 369 562 189 762 287 645 560 797 556 956 715 875 148 882 702 720 343 630 672 232 492 970 478 71 553 280 277 390 872 519 600 998 225 828 374 561 290 622 651 859 72 177 646 127 193 445 237 89 776 64 682 368 509 445 35 716 795 458 166 312 584 504 552 988 568 831 382 575 894 403 28 390 718 264 137 595 685 599 749 919 344 920 654 618 571 831 846 422 773 152 967 336 550 242 874 321 264 298 440 171 642 962 927 756 775 963 7 746 586 562 502 31 829 741 598 985 267 433 684 247 450 257 861 719 696 611 869 669 324 319 675 867 604 840 841 803 649 453 415 13 158 15 620 429 799 571 757 120 442 705 752 279 920 968 256 151 606 497 203 875 27 828 588 849 612 665 414 470 676 373 771 403 691 308 179 600 981 694 80 599 449 761 561 549 725 227 800 227 883 442 719 621 823 622 211 300 190 228 349 694 405 572 903 400 686 850 237 810 139 315 357 716 382 652 74 158 619 36 233 661 631 29 687 280 623 529 916 881 542 733 583 96 326 428 921 184 305 125 405 523 706 713 650 272 831 160 249 320 741 840 543 567 589 545 451 79 472 131 91 18 112 41 787 642 462 191 858 558 155 794 875 467 17 174 705 383 75 884 137 391 360 661 397 17 26 215 36 953 985 514 23 860 377 314 974 819 992 904 52 645 733 684 895 528 116 274 644 590 583 888 314 892 540 47 71 374 257 278 695 384 201 655 497 570 854 639 870 826 32 706 269 612 491 30 49 702 777 799 307 355 180 329 230 813 732 70 305 880 276 687 29 279 451 432 152 271 486 87 459 683 125 545 139 226 39 445 8 226 507 570 799 695 794 354 36 797 494 591 471 752 435 931 274 255 168 144 370 605 622 122 52 171 965 883 284 650 537 515 \n",
            "Sum Vector: 1197 699 928 492 626 713 1408 1045 658 826 1427 1515 1259 900 1447 1551 840 1714 1173 1058 864 1050 1686 937 510 618 1630 926 1089 1696 1043 886 851 1004 217 1232 1241 1059 1434 327 1168 1587 1137 1450 1044 818 747 676 1364 1031 1151 719 1251 377 1185 1334 356 266 1139 775 1215 1327 1355 617 1445 357 968 694 1265 961 1481 565 1273 1034 900 652 459 1135 392 1591 1458 843 1434 1703 313 558 1368 984 660 406 1236 734 447 1215 679 1085 1459 622 1169 650 1750 982 1038 114 1393 1315 1340 1030 977 1034 1108 1203 752 519 559 1462 435 132 467 931 528 900 706 711 1584 1396 1569 318 481 948 768 1405 1559 437 1545 1295 968 645 1830 414 454 1164 1115 697 1080 1397 1112 1108 1398 1106 1266 370 1302 1318 429 956 1249 612 659 369 1012 1645 1275 1205 1003 969 770 1561 1469 1613 318 1486 746 637 1464 1554 830 1010 1657 482 1584 987 1553 835 837 624 1075 427 1265 1283 184 615 742 1472 897 1117 815 892 1062 1447 881 1112 1240 1130 717 1645 309 267 590 1156 750 1026 787 991 1351 454 569 893 218 1371 847 1540 333 1264 603 116 1456 1624 765 446 760 405 579 1194 1083 1437 639 1495 1490 953 590 1151 916 642 1096 1207 749 1242 1320 831 764 1743 1950 1433 1055 942 161 1748 1222 817 1240 231 1342 1023 1319 1475 1180 1192 865 980 1069 1271 1369 1014 172 1317 1856 755 1079 1623 1042 538 725 1411 765 864 1018 419 917 1034 968 855 956 433 1797 490 1212 768 1168 1232 1274 1120 993 736 380 1705 827 879 1226 1524 1073 569 452 1279 1221 905 664 198 974 624 410 1056 1593 1078 1656 1253 1026 1071 1864 1450 1270 990 1437 1000 475 1824 1011 1179 1225 833 1269 315 1261 829 1441 1267 339 1053 150 949 384 920 1358 1408 767 1169 329 388 1223 1773 1201 136 708 1076 300 1541 1151 1488 1266 1149 492 1125 859 821 787 1155 397 1288 891 1019 799 933 79 518 1374 1435 1043 746 1131 1041 1571 963 898 1129 1025 1037 766 1481 800 544 1428 1098 597 1430 468 704 375 1064 1327 1119 399 1234 1183 1717 476 1509 1649 432 1156 1032 1040 1073 459 1096 739 547 428 1010 906 946 1396 991 1121 1052 990 1311 876 854 1579 1179 871 810 1566 1263 1955 1559 1075 1158 1381 529 648 459 1634 917 971 940 1232 1199 1331 327 730 1047 1844 705 1471 301 1188 701 1133 444 1123 1002 543 562 1370 1218 831 1609 878 1605 308 1526 1724 1024 1253 665 1910 1141 575 1033 1297 749 925 1212 1728 1210 428 887 1020 363 436 1059 1106 1856 961 1331 549 1147 1495 1335 1529 317 986 816 523 986 616 362 1303 1405 1165 932 873 598 1144 1001 184 336 579 882 1388 415 971 729 1054 1243 631 343 959 1283 1581 1130 1307 1640 676 1342 448 422 593 398 1046 924 1521 1238 1265 1288 1158 668 1647 1408 889 1092 566 814 1370 1863 910 150 110 1321 1263 927 1438 1647 610 1112 863 1245 423 645 329 618 1000 728 862 1488 1365 1306 1069 944 714 1234 965 1064 893 1592 922 1518 1036 387 354 1079 1347 578 1043 799 634 1392 867 1398 1223 1390 1256 1773 917 1246 1443 1695 767 1264 1204 625 803 1847 530 838 939 307 898 545 1297 792 1174 1589 1169 1457 623 621 508 1153 771 1552 628 316 996 695 711 1091 382 884 1591 334 832 993 833 787 195 1020 903 704 607 1249 943 1261 725 1934 897 1236 632 1127 1785 775 958 862 1541 464 1021 1171 758 1050 987 1506 1013 1022 1381 1465 1120 1205 1445 995 1689 190 1877 435 1202 1177 1526 482 483 891 710 1087 1005 1316 1767 1492 1475 1871 101 1045 1496 903 1314 266 947 1070 1219 1897 859 1380 878 436 1261 467 1670 1074 1495 1244 1420 1664 1022 1081 949 1523 898 1140 1481 1242 1292 513 1277 537 270 640 999 465 1095 671 1583 910 1155 1017 1035 787 1673 1700 943 1132 1372 663 781 1801 29 1274 724 1149 642 1219 1130 1414 1010 1163 1056 928 1421 1209 425 783 1187 1474 649 1142 494 1378 776 1132 1315 502 1297 857 1726 977 782 663 1523 938 466 873 203 642 453 824 480 1493 1108 551 1473 1692 994 1281 199 747 708 1684 930 1596 157 856 1098 118 1107 1098 903 892 1555 471 1449 1477 1897 1126 1192 1618 1199 988 880 560 1481 626 1210 338 749 929 1086 1117 1272 1218 1023 517 1015 576 1669 941 1205 591 789 590 797 147 629 923 565 773 386 220 1501 1003 990 673 1215 668 602 1249 1833 1340 716 1124 813 927 893 1660 1000 917 1312 1595 615 588 930 1026 937 987 1039 993 713 1109 476 410 1489 876 1326 1384 793 1567 1163 1544 1017 1165 348 316 783 1077 604 1144 872 951 846 196 1015 418 413 336 847 1190 364 665 694 1167 935 1098 1038 1709 442 931 724 693 1017 272 588 1468 1379 1508 581 1076 341 675 281 1320 1346 942 306 914 1072 949 706 1021 1257 862 910 1059 1170 1039 527 936 316 650 569 1003 806 711 76 950 966 1229 1458 1536 1079 1137 641 1186 655 1285 1456 921 945 1128 717 412 301 321 689 1444 786 433 710 442 1863 1645 1072 1637 1256 1096 \n",
            "\n"
          ]
        }
      ]
    }
  ]
}